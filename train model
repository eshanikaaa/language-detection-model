import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report
import joblib

# Load dataset (https://www.kaggle.com/datasets/basilb2s/language-detection)
# Load the CSV file containing the language detection data into a pandas DataFrame
dataset_path = './dataset/LanguageDetection.csv'
data = pd.read_csv(dataset_path)

# Separate the features (text) and labels (language)
X = data['Text']  # Feature set containing text data
y = data['Language']  # Labels representing the language of each text

# Split the dataset into training and testing sets
# 80% of the data will be used for training and 20% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a pipeline with TF-IDF Vectorizer and Multinomial Naive Bayes model
# Pipeline helps to chain together multiple steps, such as feature extraction and classification
# TfidfVectorizer converts the text data into numerical features
# MultinomialNB is used for classification based on these features
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 5))),  # Use character-level n-grams to capture patterns within text
    ('classifier', MultinomialNB())  # Use Naive Bayes classifier for prediction
])

# Use GridSearchCV to find the best parameters
# GridSearchCV helps to perform hyperparameter tuning to find the optimal values for parameters
param_grid = {
    'tfidf__ngram_range': [(1, 2), (2, 5)],  # Tune the n-gram range for TF-IDF
    'classifier__alpha': [0.1, 0.5, 1.0]  # Tune the smoothing parameter (alpha) for Naive Bayes
}

# Perform a grid search over the parameter grid using 3-fold cross-validation
# n_jobs=-1 utilizes all available CPU cores, and verbose=2 shows progress output
grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Get the best model found by GridSearchCV
model = grid_search.best_estimator_

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model's performance on the test data
# Print the accuracy and a classification report to show detailed performance metrics
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')
print('Classification Report:')
print(classification_report(y_test, y_pred))

# Save the trained model for future use
# Save the model using joblib to a file, so that it can be loaded later without retraining
joblib.dump(model, './model/language_detection_model.pkl')

print("Model training complete and saved as 'language_detection_model.pkl'")
